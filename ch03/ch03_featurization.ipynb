{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ranking-father",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master('local[*]') \\\n",
    "    .appName(\"Intro\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "frank-operations",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "training = spark.read.csv ('training_bot_data.csv', header= True) \n",
    "#testing is not needed as it is not classified and is useless forus\n",
    "#testing = spark.read.csv ('testing_bot_data.csv', header= True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "increased-opportunity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# union\n",
    "data = training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "infrared-mobile",
   "metadata": {},
   "outputs": [],
   "source": [
    "# provide schema:\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.sql.types import IntegerType, BooleanType\n",
    "\n",
    "casted_df = data.withColumn(\"friends_count\", data[\"friends_count\"].cast(IntegerType()))\n",
    "casted_df = casted_df.withColumn(\"listed_count\", casted_df[\"listed_count\"].cast(IntegerType()))\n",
    "casted_df = casted_df.withColumn(\"favourites_count\", casted_df[\"favourites_count\"].cast(IntegerType()))\n",
    "casted_df = casted_df.withColumn(\"statuses_count\", casted_df[\"statuses_count\"].cast(IntegerType()))\n",
    "casted_df = casted_df.withColumn(\"verified\", casted_df[\"verified\"].cast(BooleanType()))\n",
    "casted_df = casted_df.withColumn(\"default_profile\", casted_df[\"default_profile\"].cast(BooleanType()))\n",
    "casted_df = casted_df.withColumn(\"has_extended_profile\", casted_df[\"has_extended_profile\"].cast(BooleanType()))\n",
    "casted_df = casted_df.withColumn(\"default_profile_image\", casted_df[\"default_profile_image\"].cast(BooleanType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "understood-taxation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- id_str: string (nullable = true)\n",
      " |-- screen_name: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- followers_count: string (nullable = true)\n",
      " |-- friends_count: integer (nullable = true)\n",
      " |-- listed_count: integer (nullable = true)\n",
      " |-- created_at: string (nullable = true)\n",
      " |-- favourites_count: integer (nullable = true)\n",
      " |-- verified: boolean (nullable = true)\n",
      " |-- statuses_count: integer (nullable = true)\n",
      " |-- lang: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- default_profile: boolean (nullable = true)\n",
      " |-- default_profile_image: boolean (nullable = true)\n",
      " |-- has_extended_profile: boolean (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- bot: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "casted_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "upset-kelly",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# try new data for the fun of it:\n",
    "\n",
    "users = spark.read.csv ('users.csv', header= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "thick-lobby",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id='24858289', name='Davide Bertoli', screen_name='davideb66', statuses_count='1299', followers_count='22', friends_count='40', favourites_count='1', listed_count='0', url=None, lang='it', time_zone='Rome', location=None, default_profile='1', default_profile_image='1', geo_enabled='1', profile_image_url='http://abs.twimg.com/sticky/default_profile_images/default_profile_1_normal.png', profile_banner_url='NULL', profile_use_background_image='1', profile_background_image_url_https='https://abs.twimg.com/images/themes/theme1/bg.png', profile_text_color='333333', profile_image_url_https='https://abs.twimg.com/sticky/default_profile_images/default_profile_1_normal.png', profile_sidebar_border_color='C0DEED', profile_background_tile=None, profile_sidebar_fill_color='DDEEF6', profile_background_image_url='http://abs.twimg.com/images/themes/theme1/bg.png', profile_background_color='C0DEED', profile_link_color='0084B4', utc_offset='7200', is_translator=None, follow_request_sent=None, protected=None, verified=None, notifications=None, description=None, contributors_enabled=None, following=None, created_at='Tue Mar 17 08:51:12 +0000 2009', timestamp='2009-03-17 09:51:12', crawled_at='2014-04-19 14:46:19', updated='2016-03-15 14:12:22', test_set_1='1')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "million-ecuador",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- screen_name: string (nullable = true)\n",
      " |-- statuses_count: string (nullable = true)\n",
      " |-- followers_count: string (nullable = true)\n",
      " |-- friends_count: string (nullable = true)\n",
      " |-- favourites_count: string (nullable = true)\n",
      " |-- listed_count: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- lang: string (nullable = true)\n",
      " |-- time_zone: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- default_profile: string (nullable = true)\n",
      " |-- default_profile_image: string (nullable = true)\n",
      " |-- geo_enabled: string (nullable = true)\n",
      " |-- profile_image_url: string (nullable = true)\n",
      " |-- profile_banner_url: string (nullable = true)\n",
      " |-- profile_use_background_image: string (nullable = true)\n",
      " |-- profile_background_image_url_https: string (nullable = true)\n",
      " |-- profile_text_color: string (nullable = true)\n",
      " |-- profile_image_url_https: string (nullable = true)\n",
      " |-- profile_sidebar_border_color: string (nullable = true)\n",
      " |-- profile_background_tile: string (nullable = true)\n",
      " |-- profile_sidebar_fill_color: string (nullable = true)\n",
      " |-- profile_background_image_url: string (nullable = true)\n",
      " |-- profile_background_color: string (nullable = true)\n",
      " |-- profile_link_color: string (nullable = true)\n",
      " |-- utc_offset: string (nullable = true)\n",
      " |-- is_translator: string (nullable = true)\n",
      " |-- follow_request_sent: string (nullable = true)\n",
      " |-- protected: string (nullable = true)\n",
      " |-- verified: string (nullable = true)\n",
      " |-- notifications: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- contributors_enabled: string (nullable = true)\n",
      " |-- following: string (nullable = true)\n",
      " |-- created_at: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- crawled_at: string (nullable = true)\n",
      " |-- updated: string (nullable = true)\n",
      " |-- test_set_1: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "incorporate-teach",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id='532627591686275072', text='I Pooh - In silenzio 1968 http://t.co/ahvQxUqTws', source='\"<a href=\"\"http://www.facebook.com/twitter\"\" rel=\"\"nofollow\"\">Facebook</a>\"', user_id='24858289', truncated=None, in_reply_to_status_id='0', in_reply_to_user_id='0', in_reply_to_screen_name=None, retweeted_status_id='0', geo=None, place=None, contributors=None, retweet_count='0', reply_count='0', favorite_count='0', favorited=None, retweeted=None, possibly_sensitive=None, num_hashtags='0', num_urls='1', num_mentions='0', created_at='Wed Nov 12 20:14:48 +0000 2014', timestamp='2014-11-12 21:14:48', crawled_at='2014-11-12 21:44:09', updated='2014-11-12 21:44:09'),\n",
       " Row(id='532624255058706432', text='http://t.co/HyI5EQKz6Q', source='\"<a href=\"\"http://www.facebook.com/twitter\"\" rel=\"\"nofollow\"\">Facebook</a>\"', user_id='24858289', truncated=None, in_reply_to_status_id='0', in_reply_to_user_id='0', in_reply_to_screen_name=None, retweeted_status_id='0', geo=None, place=None, contributors=None, retweet_count='0', reply_count='0', favorite_count='0', favorited=None, retweeted=None, possibly_sensitive=None, num_hashtags='0', num_urls='1', num_mentions='0', created_at='Wed Nov 12 20:01:32 +0000 2014', timestamp='2014-11-12 21:01:32', crawled_at='2014-11-12 21:44:09', updated='2014-11-12 21:44:09')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = spark.read.csv ('tweets.csv', header= True)\n",
    "tweets.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "shaped-microphone",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- source: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- truncated: string (nullable = true)\n",
      " |-- in_reply_to_status_id: string (nullable = true)\n",
      " |-- in_reply_to_user_id: string (nullable = true)\n",
      " |-- in_reply_to_screen_name: string (nullable = true)\n",
      " |-- retweeted_status_id: string (nullable = true)\n",
      " |-- geo: string (nullable = true)\n",
      " |-- place: string (nullable = true)\n",
      " |-- contributors: string (nullable = true)\n",
      " |-- retweet_count: string (nullable = true)\n",
      " |-- reply_count: string (nullable = true)\n",
      " |-- favorite_count: string (nullable = true)\n",
      " |-- favorited: string (nullable = true)\n",
      " |-- retweeted: string (nullable = true)\n",
      " |-- possibly_sensitive: string (nullable = true)\n",
      " |-- num_hashtags: string (nullable = true)\n",
      " |-- num_urls: string (nullable = true)\n",
      " |-- num_mentions: string (nullable = true)\n",
      " |-- created_at: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- crawled_at: string (nullable = true)\n",
      " |-- updated: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "assigned-penalty",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "\n",
    "specific_user = tweets.where(f.col(\"user_id\")=='24858289')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "prescribed-brake",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2518"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "specific_user.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "latter-barbados",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id='532627591686275072', text='I Pooh - In silenzio 1968 http://t.co/ahvQxUqTws', source='\"<a href=\"\"http://www.facebook.com/twitter\"\" rel=\"\"nofollow\"\">Facebook</a>\"', user_id='24858289', truncated=None, in_reply_to_status_id='0', in_reply_to_user_id='0', in_reply_to_screen_name=None, retweeted_status_id='0', geo=None, place=None, contributors=None, retweet_count='0', reply_count='0', favorite_count='0', favorited=None, retweeted=None, possibly_sensitive=None, num_hashtags='0', num_urls='1', num_mentions='0', created_at='Wed Nov 12 20:14:48 +0000 2014', timestamp='2014-11-12 21:14:48', crawled_at='2014-11-12 21:44:09', updated='2014-11-12 21:44:09'),\n",
       " Row(id='532624255058706432', text='http://t.co/HyI5EQKz6Q', source='\"<a href=\"\"http://www.facebook.com/twitter\"\" rel=\"\"nofollow\"\">Facebook</a>\"', user_id='24858289', truncated=None, in_reply_to_status_id='0', in_reply_to_user_id='0', in_reply_to_screen_name=None, retweeted_status_id='0', geo=None, place=None, contributors=None, retweet_count='0', reply_count='0', favorite_count='0', favorited=None, retweeted=None, possibly_sensitive=None, num_hashtags='0', num_urls='1', num_mentions='0', created_at='Wed Nov 12 20:01:32 +0000 2014', timestamp='2014-11-12 21:01:32', crawled_at='2014-11-12 21:44:09', updated='2014-11-12 21:44:09'),\n",
       " Row(id='532513524460052480', text='Tutti a tavola, con il filetto di baccalà. http://t.co/aHHbFXJbIS', source='\"<a href=\"\"http://www.facebook.com/twitter\"\" rel=\"\"nofollow\"\">Facebook</a>\"', user_id='24858289', truncated=None, in_reply_to_status_id='0', in_reply_to_user_id='0', in_reply_to_screen_name=None, retweeted_status_id='0', geo=None, place=None, contributors=None, retweet_count='0', reply_count='0', favorite_count='0', favorited=None, retweeted=None, possibly_sensitive=None, num_hashtags='0', num_urls='1', num_mentions='0', created_at='Wed Nov 12 12:41:32 +0000 2014', timestamp='2014-11-12 13:41:32', crawled_at='2014-11-12 21:44:09', updated='2014-11-12 21:44:09'),\n",
       " Row(id='532297646669852672', text='http://t.co/NAHQ4l2pUy', source='\"<a href=\"\"http://www.facebook.com/twitter\"\" rel=\"\"nofollow\"\">Facebook</a>\"', user_id='24858289', truncated=None, in_reply_to_status_id='0', in_reply_to_user_id='0', in_reply_to_screen_name=None, retweeted_status_id='0', geo=None, place=None, contributors=None, retweet_count='0', reply_count='0', favorite_count='0', favorited=None, retweeted=None, possibly_sensitive=None, num_hashtags='0', num_urls='1', num_mentions='0', created_at='Tue Nov 11 22:23:43 +0000 2014', timestamp='2014-11-11 23:23:43', crawled_at='2014-11-12 21:44:09', updated='2014-11-12 21:44:09'),\n",
       " Row(id='532295960807100416', text='Gold - Spandau Ballet http://t.co/o8ZJHt7Neu', source='\"<a href=\"\"http://www.facebook.com/twitter\"\" rel=\"\"nofollow\"\">Facebook</a>\"', user_id='24858289', truncated=None, in_reply_to_status_id='0', in_reply_to_user_id='0', in_reply_to_screen_name=None, retweeted_status_id='0', geo=None, place=None, contributors=None, retweet_count='0', reply_count='0', favorite_count='0', favorited=None, retweeted=None, possibly_sensitive=None, num_hashtags='0', num_urls='1', num_mentions='0', created_at='Tue Nov 11 22:17:01 +0000 2014', timestamp='2014-11-11 23:17:01', crawled_at='2014-11-12 21:44:09', updated='2014-11-12 21:44:09')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "specific_user.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "acknowledged-cardiff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id='123', name='test', age='14')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = spark.read.csv ('bot_or_not - Sheet1.csv', header= True)\n",
    "test.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "interested-highway",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "informal-australia",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'friends_count'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-1c95a68d1570>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# compute statistics for multiple metrics with weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mcasted_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummarizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfriends_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# # compute statistics for multiple metrics without weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1641\u001b[0m         \"\"\"\n\u001b[1;32m   1642\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1643\u001b[0;31m             raise AttributeError(\n\u001b[0m\u001b[1;32m   1644\u001b[0m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[1;32m   1645\u001b[0m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'friends_count'"
     ]
    }
   ],
   "source": [
    "# from pyspark.ml.stat import Summarizer\n",
    "# from pyspark.sql import Row\n",
    "# from pyspark.ml.linalg import Vectors\n",
    "\n",
    "\n",
    "\n",
    "# # create summarizer for multiple metrics \"mean\" and \"count\"\n",
    "# summarizer = Summarizer.metrics(\"mean\")\n",
    "\n",
    "# # compute statistics for multiple metrics with weight\n",
    "# casted_df.select(summarizer.summary(tweets.friends_count)).show(truncate=False)\n",
    "\n",
    "# # # compute statistics for multiple metrics without weight\n",
    "# # df.select(summarizer.summary(df.features)).show(truncate=False)\n",
    "\n",
    "# # # compute statistics for single metric \"mean\" with weight\n",
    "# # df.select(Summarizer.mean(df.features, df.weight)).show(truncate=False)\n",
    "\n",
    "# # # compute statistics for single metric \"mean\" without weight\n",
    "# # df.select(Summarizer.mean(df.features)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "stylish-jimmy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: [Hi, I, heard, about, Spark] => \n",
      "Vector: [0.03810776919126511,-0.0128551559522748,-0.007444112002849579]\n",
      "\n",
      "Text: [I, wish, Java, could, use, case, classes] => \n",
      "Vector: [-0.023378989260111536,-0.012126037584883825,0.04162612025226865]\n",
      "\n",
      "Text: [Logistic, regression, models, are, neat] => \n",
      "Vector: [0.009750119224190713,0.039336619153618815,-0.03333115838468075]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "\n",
    "# Input data: Each row is a bag of words from a sentence or document.\n",
    "documentDF = spark.createDataFrame([\n",
    "    (\"Hi I heard about Spark\".split(\" \"), ),\n",
    "    (\"I wish Java could use case classes\".split(\" \"), ),\n",
    "    (\"Logistic regression models are neat\".split(\" \"), )\n",
    "], [\"text\"])\n",
    "\n",
    "# Learn a mapping from words to Vectors.\n",
    "word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"text\", outputCol=\"result\")\n",
    "model = word2Vec.fit(documentDF)\n",
    "\n",
    "result = model.transform(documentDF)\n",
    "for row in result.collect():\n",
    "    text, vector = row\n",
    "    print(\"Text: [%s] => \\nVector: %s\\n\" % (\", \".join(text), str(vector)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "express-american",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+-------------------------+\n",
      "|id |words          |features                 |\n",
      "+---+---------------+-------------------------+\n",
      "|0  |[a, b, c]      |(3,[0,1,2],[1.0,1.0,1.0])|\n",
      "|1  |[a, b, b, c, a]|(3,[0,1,2],[2.0,2.0,1.0])|\n",
      "+---+---------------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "# Input data: Each row is a bag of words with a ID.\n",
    "df = spark.createDataFrame([\n",
    "    (0, \"a b c\".split(\" \")),\n",
    "    (1, \"a b b c a\".split(\" \"))\n",
    "], [\"id\", \"words\"])\n",
    "\n",
    "# fit a CountVectorizerModel from the corpus.\n",
    "cv = CountVectorizer(inputCol=\"words\", outputCol=\"features\", vocabSize=10, minDF=2.0)\n",
    "\n",
    "model = cv.fit(df)\n",
    "\n",
    "result = model.transform(df)\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "english-rabbit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------------+------+-------------------------------------------------------+\n",
      "|real|bool |stringNum       |string|features                                               |\n",
      "+----+-----+----------------+------+-------------------------------------------------------+\n",
      "|2.2 |true |let the fun     |foo   |(262144,[45335,174475,247670,257907],[1.0,2.2,1.0,1.0])|\n",
      "|3.3 |false|today is the day|bar   |(262144,[70644,133008,173866,174475],[1.0,1.0,1.0,3.3])|\n",
      "|4.4 |false|3               |baz   |(262144,[22406,70644,174475,187923],[1.0,1.0,4.4,1.0]) |\n",
      "|5.5 |false|4               |foo   |(262144,[70644,101499,174475,257907],[1.0,1.0,5.5,1.0])|\n",
      "+----+-----+----------------+------+-------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import FeatureHasher\n",
    "\n",
    "dataset = spark.createDataFrame([\n",
    "    (2.2, True, \"let the fun\", \"foo\"),\n",
    "    (3.3, False, \"today is the day\", \"bar\"),\n",
    "    (4.4, False, \"3\", \"baz\"),\n",
    "    (5.5, False, \"4\", \"foo\")\n",
    "], [\"real\", \"bool\", \"stringNum\", \"string\"])\n",
    "\n",
    "hasher = FeatureHasher(inputCols=[\"real\", \"bool\", \"stringNum\", \"string\"],\n",
    "                       outputCol=\"features\")\n",
    "\n",
    "featurized = hasher.transform(dataset)\n",
    "featurized.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "parallel-quilt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------+------------------------------------------------------------+------+\n",
      "|sentence                                   |words                                                       |tokens|\n",
      "+-------------------------------------------+------------------------------------------------------------+------+\n",
      "|Hi|I|heard|about|Spark                     |[hi|i|heard|about|spark]                                    |1     |\n",
      "|I     wish Java      could use case classes|[i, , , , , wish, java, , , , , , could, use, case, classes]|16    |\n",
      "|Logistic,regression,models,are,neat        |[logistic,regression,models,are,neat]                       |1     |\n",
      "+-------------------------------------------+------------------------------------------------------------+------+\n",
      "\n",
      "+-------------------------------------------+------------------------------------------+------+\n",
      "|sentence                                   |words                                     |tokens|\n",
      "+-------------------------------------------+------------------------------------------+------+\n",
      "|Hi|I|heard|about|Spark                     |[hi, i, heard, about, spark]              |5     |\n",
      "|I     wish Java      could use case classes|[i, wish, java, could, use, case, classes]|7     |\n",
      "|Logistic,regression,models,are,neat        |[logistic, regression, models, are, neat] |5     |\n",
      "+-------------------------------------------+------------------------------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "sentenceDataFrame = spark.createDataFrame([\n",
    "    (0, \"Hi|I|heard|about|Spark\"),\n",
    "    (1, \"I     wish Java      could use case classes\"),\n",
    "    (2, \"Logistic,regression,models,are,neat\")\n",
    "], [\"id\", \"sentence\"])\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"sentence\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "# alternatively, pattern=\"\\\\w+\", gaps(False)\n",
    "\n",
    "countTokens = udf(lambda words: len(words), IntegerType())\n",
    "\n",
    "tokenized = tokenizer.transform(sentenceDataFrame)\n",
    "tokenized.select(\"sentence\", \"words\")\\\n",
    "    .withColumn(\"tokens\", countTokens(col(\"words\"))).show(truncate=False)\n",
    "\n",
    "regexTokenized = regexTokenizer.transform(sentenceDataFrame)\n",
    "regexTokenized.select(\"sentence\", \"words\") \\\n",
    "    .withColumn(\"tokens\", countTokens(col(\"words\"))).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "multiple-tender",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------------------+--------------------+\n",
      "|id |raw                         |filtered            |\n",
      "+---+----------------------------+--------------------+\n",
      "|0  |[I, saw, the, red, balloon] |[saw, red, balloon] |\n",
      "|1  |[Mary, had, a, little, lamb]|[Mary, little, lamb]|\n",
      "+---+----------------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "sentenceData = spark.createDataFrame([\n",
    "    (0, [\"I\", \"saw\", \"the\", \"red\", \"balloon\"]),\n",
    "    (1, [\"Mary\", \"had\", \"a\", \"little\", \"lamb\"])\n",
    "], [\"id\", \"raw\"])\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"raw\", outputCol=\"filtered\")\n",
    "remover.transform(sentenceData).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annoying-browser",
   "metadata": {},
   "source": [
    "# Code Example -> Tokanizer -> N Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "figured-render",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------+\n",
      "|ngrams                                                           |\n",
      "+-----------------------------------------------------------------+\n",
      "|[hi i, i heard, heard about, about spark]                        |\n",
      "|[i wish,, wish, wish, wish java,, java, java, java could]        |\n",
      "|[logistic regression,, regression, regression, regression models]|\n",
      "+-----------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import NGram\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "sentenceDataFrame = spark.createDataFrame([\n",
    "    (0, \"Hi I heard about Spark \"),\n",
    "    (1, \"I wish, wish Java, Java could\"),\n",
    "    (2, \"Logistic regression, regression models\")\n",
    "], [\"id\", \"sentence\"])\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "\n",
    "countTokens = udf(lambda words: len(words), IntegerType())\n",
    "\n",
    "wordDataFrame = tokenizer.transform(sentenceDataFrame)\n",
    "\n",
    "\n",
    "ngram = NGram(n=2, inputCol=\"words\", outputCol=\"ngrams\")\n",
    "\n",
    "ngramDataFrame = ngram.transform(wordDataFrame)\n",
    "ngramDataFrame.select(\"ngrams\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "physical-southeast",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binarizer output with Threshold = 0.500000\n",
      "+---+-------+-----------------+\n",
      "| id|feature|binarized_feature|\n",
      "+---+-------+-----------------+\n",
      "|  0|    5.1|              1.0|\n",
      "|  1|    5.8|              1.0|\n",
      "|  2|    0.2|              0.0|\n",
      "+---+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Binarizer\n",
    "\n",
    "continuousDataFrame = spark.createDataFrame([\n",
    "    (0, 5.1),\n",
    "    (1, 5.8),\n",
    "    (2, 0.2)\n",
    "], [\"id\", \"feature\"])\n",
    "\n",
    "binarizer = Binarizer(threshold=0.5, inputCol=\"feature\", outputCol=\"binarized_feature\")\n",
    "\n",
    "binarizedDataFrame = binarizer.transform(continuousDataFrame)\n",
    "\n",
    "print(\"Binarizer output with Threshold = %f\" % binarizer.getThreshold())\n",
    "binarizedDataFrame.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "adequate-statistics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------+\n",
      "|pcaFeatures                                                |\n",
      "+-----------------------------------------------------------+\n",
      "|[1.6485728230883807,-4.013282700516296,-5.524543751369388] |\n",
      "|[-4.645104331781534,-1.1167972663619026,-5.524543751369387]|\n",
      "|[-6.428880535676489,-5.337951427775355,-5.524543751369389] |\n",
      "+-----------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "data = [(Vectors.sparse(5, [(1, 1.0), (3, 7.0)]),),\n",
    "        (Vectors.dense([2.0, 0.0, 3.0, 4.0, 5.0]),),\n",
    "        (Vectors.dense([4.0, 0.0, 0.0, 6.0, 7.0]),)]\n",
    "df = spark.createDataFrame(data, [\"features\"])\n",
    "\n",
    "pca = PCA(k=3, inputCol=\"features\", outputCol=\"pcaFeatures\")\n",
    "model = pca.fit(df)\n",
    "\n",
    "result = model.transform(df).select(\"pcaFeatures\")\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "worldwide-miracle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------------------------------------------------------------------------+\n",
      "|features |polyFeatures                                                                        |\n",
      "+---------+------------------------------------------------------------------------------------+\n",
      "|[2.0,1.0]|[2.0,4.0,8.0,16.0,32.0,1.0,2.0,4.0,8.0,16.0,1.0,2.0,4.0,8.0,1.0,2.0,4.0,1.0,2.0,1.0]|\n",
      "|[0.0,0.0]|[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]   |\n",
      "+---------+------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import PolynomialExpansion\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (Vectors.dense([2.0, 1.0]),),\n",
    "    (Vectors.dense([0.0, 0.0]),)\n",
    "], [\"features\"])\n",
    "\n",
    "polyExpansion = PolynomialExpansion(degree=5, inputCol=\"features\", outputCol=\"polyFeatures\")\n",
    "polyDF = polyExpansion.transform(df)\n",
    "\n",
    "polyDF.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "parental-battlefield",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------+\n",
      "|featuresDCT                                                     |\n",
      "+----------------------------------------------------------------+\n",
      "|[1.0,-1.1480502970952693,2.0000000000000004,-2.7716385975338604]|\n",
      "|[-1.0,3.378492794482933,-7.000000000000001,2.9301512653149677]  |\n",
      "|[4.0,9.304453421915744,11.000000000000002,1.5579302036357163]   |\n",
      "+----------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import DCT\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (Vectors.dense([0.0, 1.0, -2.0, 3.0]),),\n",
    "    (Vectors.dense([-1.0, 2.0, 4.0, -7.0]),),\n",
    "    (Vectors.dense([14.0, -2.0, -5.0, 1.0]),)], [\"features\"])\n",
    "\n",
    "dct = DCT(inverse=False, inputCol=\"features\", outputCol=\"featuresDCT\")\n",
    "\n",
    "dctDf = dct.transform(df)\n",
    "\n",
    "dctDf.select(\"featuresDCT\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "alien-society",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+-------------+-------------+\n",
      "|categoryIndex1|categoryIndex2| categoryVec1| categoryVec2|\n",
      "+--------------+--------------+-------------+-------------+\n",
      "|           0.0|           1.0|(2,[0],[1.0])|(2,[1],[1.0])|\n",
      "|           1.0|           0.0|(2,[1],[1.0])|(2,[0],[1.0])|\n",
      "|           2.0|           1.0|    (2,[],[])|(2,[1],[1.0])|\n",
      "|           0.0|           2.0|(2,[0],[1.0])|    (2,[],[])|\n",
      "|           0.0|           1.0|(2,[0],[1.0])|(2,[1],[1.0])|\n",
      "|           2.0|           0.0|    (2,[],[])|(2,[0],[1.0])|\n",
      "+--------------+--------------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (0.0, 1.0),\n",
    "    (1.0, 0.0),\n",
    "    (2.0, 1.0),\n",
    "    (0.0, 2.0),\n",
    "    (0.0, 1.0),\n",
    "    (2.0, 0.0)\n",
    "], [\"categoryIndex1\", \"categoryIndex2\"])\n",
    "\n",
    "encoder = OneHotEncoder(inputCols=[\"categoryIndex1\", \"categoryIndex2\"],\n",
    "                        outputCols=[\"categoryVec1\", \"categoryVec2\"])\n",
    "model = encoder.fit(df)\n",
    "encoded = model.transform(df)\n",
    "encoded.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "exterior-thursday",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Path does not exist: file:/home/jovyan/data/mllib/sample_libsvm_data.txt",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-13f4ac0d3d86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVectorIndexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"libsvm\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/mllib/sample_libsvm_data.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVectorIndexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"features\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"indexed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxCategories\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Path does not exist: file:/home/jovyan/data/mllib/sample_libsvm_data.txt"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorIndexer\n",
    "\n",
    "data = spark.read.format(\"libsvm\").load(\"data/mllib/sample_libsvm_data.txt\")\n",
    "\n",
    "indexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexed\", maxCategories=10)\n",
    "indexerModel = indexer.fit(data)\n",
    "\n",
    "categoricalFeatures = indexerModel.categoryMaps\n",
    "print(\"Chose %d categorical features: %s\" %\n",
    "      (len(categoricalFeatures), \", \".join(str(k) for k in categoricalFeatures.keys())))\n",
    "\n",
    "# Create new column \"indexed\" with categorical values transformed to indices\n",
    "indexedData = indexerModel.transform(data)\n",
    "indexedData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "sporting-nightmare",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+--------------+\n",
      "|id1|vec1          |vec2          |\n",
      "+---+--------------+--------------+\n",
      "|1  |[1.0,2.0,3.0] |[8.0,4.0,5.0] |\n",
      "|2  |[4.0,3.0,8.0] |[7.0,9.0,8.0] |\n",
      "|3  |[6.0,1.0,9.0] |[2.0,3.0,6.0] |\n",
      "|4  |[10.0,8.0,6.0]|[9.0,4.0,5.0] |\n",
      "|5  |[9.0,2.0,7.0] |[10.0,7.0,3.0]|\n",
      "|6  |[1.0,1.0,4.0] |[2.0,8.0,4.0] |\n",
      "+---+--------------+--------------+\n",
      "\n",
      "+---+--------------+--------------+------------------------------------------------------+\n",
      "|id1|vec1          |vec2          |interactedCol                                         |\n",
      "+---+--------------+--------------+------------------------------------------------------+\n",
      "|1  |[1.0,2.0,3.0] |[8.0,4.0,5.0] |[8.0,4.0,5.0,16.0,8.0,10.0,24.0,12.0,15.0]            |\n",
      "|2  |[4.0,3.0,8.0] |[7.0,9.0,8.0] |[56.0,72.0,64.0,42.0,54.0,48.0,112.0,144.0,128.0]     |\n",
      "|3  |[6.0,1.0,9.0] |[2.0,3.0,6.0] |[36.0,54.0,108.0,6.0,9.0,18.0,54.0,81.0,162.0]        |\n",
      "|4  |[10.0,8.0,6.0]|[9.0,4.0,5.0] |[360.0,160.0,200.0,288.0,128.0,160.0,216.0,96.0,120.0]|\n",
      "|5  |[9.0,2.0,7.0] |[10.0,7.0,3.0]|[450.0,315.0,135.0,100.0,70.0,30.0,350.0,245.0,105.0] |\n",
      "|6  |[1.0,1.0,4.0] |[2.0,8.0,4.0] |[12.0,48.0,24.0,12.0,48.0,24.0,48.0,192.0,96.0]       |\n",
      "+---+--------------+--------------+------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Interaction, VectorAssembler\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    [(1, 1, 2, 3, 8, 4, 5),\n",
    "     (2, 4, 3, 8, 7, 9, 8),\n",
    "     (3, 6, 1, 9, 2, 3, 6),\n",
    "     (4, 10, 8, 6, 9, 4, 5),\n",
    "     (5, 9, 2, 7, 10, 7, 3),\n",
    "     (6, 1, 1, 4, 2, 8, 4)],\n",
    "    [\"id1\", \"id2\", \"id3\", \"id4\", \"id5\", \"id6\", \"id7\"])\n",
    "\n",
    "assembler1 = VectorAssembler(inputCols=[\"id2\", \"id3\", \"id4\"], outputCol=\"vec1\")\n",
    "\n",
    "assembled1 = assembler1.transform(df)\n",
    "\n",
    "assembler2 = VectorAssembler(inputCols=[\"id5\", \"id6\", \"id7\"], outputCol=\"vec2\")\n",
    "\n",
    "assembled2 = assembler2.transform(assembled1).select(\"id1\", \"vec1\", \"vec2\")\n",
    "\n",
    "assembled2.show(truncate=False)\n",
    "\n",
    " \n",
    "interaction = Interaction(inputCols=[\"id1\", \"vec1\", \"vec2\"], outputCol=\"interactedCol\")\n",
    "interacted = interaction.transform(assembled2)\n",
    "\n",
    "interacted.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "august-factory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized using L^1 norm\n",
      "+---+---------------+--------------------+\n",
      "| id|       features|        normFeatures|\n",
      "+---+---------------+--------------------+\n",
      "|  0| [1.0,0.5,-1.0]|[0.66666666666666...|\n",
      "|  1|[200.0,1.0,1.0]|[0.99997500093746...|\n",
      "|  2| [4.0,10.0,2.0]|[0.36514837167011...|\n",
      "+---+---------------+--------------------+\n",
      "\n",
      "Normalized using L^inf norm\n",
      "+---+---------------+-----------------+\n",
      "| id|       features|     normFeatures|\n",
      "+---+---------------+-----------------+\n",
      "|  0| [1.0,0.5,-1.0]|   [1.0,0.5,-1.0]|\n",
      "|  1|[200.0,1.0,1.0]|[1.0,0.005,0.005]|\n",
      "|  2| [4.0,10.0,2.0]|    [0.4,1.0,0.2]|\n",
      "+---+---------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Normalizer\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "dataFrame = spark.createDataFrame([\n",
    "    (0, Vectors.dense([int(\"1\"), 0.5, -1.0]),),\n",
    "    (1, Vectors.dense([200.0, 1.0, 1.0]),),\n",
    "    (2, Vectors.dense([4.0, 10.0, 2.0]),)\n",
    "], [\"id\", \"features\"])\n",
    "\n",
    "# Normalize each Vector using $L^1$ norm.\n",
    "normalizer = Normalizer(inputCol=\"features\", outputCol=\"normFeatures\", p=2.0)\n",
    "l1NormData = normalizer.transform(dataFrame)\n",
    "print(\"Normalized using L^1 norm\")\n",
    "l1NormData.show()\n",
    "\n",
    "# Normalize each Vector using $L^\\infty$ norm.\n",
    "lInfNormData = normalizer.transform(dataFrame, {normalizer.p: float(\"inf\")})\n",
    "print(\"Normalized using L^inf norm\")\n",
    "lInfNormData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "executed-investment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|        a|      scaledFeatures|\n",
      "+---------+--------------------+\n",
      "|[0.0,0.0]|           [0.0,0.0]|\n",
      "|[2.0,0.0]|[1.41421356237309...|\n",
      "+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "dataFrame =  spark.createDataFrame([(Vectors.dense([0.0,0.0]),), (Vectors.dense([2.0,0.0]),)], [\"a\"])\n",
    "scaler = StandardScaler(inputCol=\"a\", outputCol=\"scaledFeatures\",\n",
    "                        withStd=True, withMean=False)\n",
    "\n",
    "# Compute summary statistics by fitting the StandardScaler\n",
    "scalerModel = scaler.fit(dataFrame)\n",
    "\n",
    "# Normalize each feature to have unit standard deviation.\n",
    "scaledData = scalerModel.transform(dataFrame)\n",
    "scaledData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "certified-frontier",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------+\n",
      "| id|  features|scaledFeatures|\n",
      "+---+----------+--------------+\n",
      "|  0| [0.0,0.0]|     [0.0,0.0]|\n",
      "|  1|[1.0,-1.0]|    [0.5,-0.5]|\n",
      "|  2|[2.0,-2.0]|    [1.0,-1.0]|\n",
      "|  3|[3.0,-3.0]|    [1.5,-1.5]|\n",
      "|  4|[4.0,-4.0]|    [2.0,-2.0]|\n",
      "+---+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "data = [(0, Vectors.dense([0.0, 0.0]),),\n",
    "        (1, Vectors.dense([1.0, -1.0]),),\n",
    "        (2, Vectors.dense([2.0, -2.0]),),\n",
    "        (3, Vectors.dense([3.0, -3.0]),),\n",
    "        (4, Vectors.dense([4.0, -4.0]),),]\n",
    "df = spark.createDataFrame(data, [\"id\", \"features\"])\n",
    "\n",
    "\n",
    "scaler = RobustScaler(inputCol=\"features\", outputCol=\"scaledFeatures\",\n",
    "                      withScaling=True, withCentering=False,\n",
    "                      lower=0.25, upper=0.75)\n",
    "\n",
    "# Compute summary statistics by fitting the RobustScaler\n",
    "scalerModel = scaler.fit(df)\n",
    "\n",
    "# Transform each feature to have unit quantile range.\n",
    "scaledData = scalerModel.transform(df)\n",
    "scaledData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "drawn-review",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|            features|      scaledFeatures|\n",
      "+--------------------+--------------------+\n",
      "|[1.0,0.1,-8.0,200.0]|[0.25,0.010000000...|\n",
      "|  [2.0,1.0,-4.0,2.0]| [0.5,0.1,-0.5,0.01]|\n",
      "|  [4.0,10.0,8.0,0.0]|   [1.0,1.0,1.0,0.0]|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import MaxAbsScaler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "dataFrame = spark.createDataFrame([\n",
    "    (0, Vectors.dense([1.0, 0.1, -8.0,200]),),\n",
    "    (1, Vectors.dense([2.0, 1.0, -4.0,2]),),\n",
    "    (2, Vectors.dense([4.0, 10.0, 8.0,0]),)\n",
    "], [\"id\", \"features\"])\n",
    "\n",
    "scaler = MaxAbsScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "\n",
    "# Compute summary statistics and generate MaxAbsScalerModel\n",
    "scalerModel = scaler.fit(dataFrame)\n",
    "\n",
    "# rescale each feature to range [-1, 1].\n",
    "scaledData = scalerModel.transform(dataFrame)\n",
    "\n",
    "scaledData.select(\"features\", \"scaledFeatures\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "champion-lyric",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucketizer output with 4 buckets\n",
      "+--------+----------------+\n",
      "|features|bucketedFeatures|\n",
      "+--------+----------------+\n",
      "|  -999.9|             0.0|\n",
      "|    -0.5|             1.0|\n",
      "|    -0.3|             1.0|\n",
      "|     0.0|             2.0|\n",
      "|     0.2|             2.0|\n",
      "|   999.9|             3.0|\n",
      "+--------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Bucketizer\n",
    "\n",
    "splits = [-float(\"inf\"), -0.5, 0.0, 0.5, float(\"inf\")]\n",
    "\n",
    "data = [(-999.9,), (-0.5,), (-0.3,), (0.0,), (0.2,), (999.9,)]\n",
    "dataFrame = spark.createDataFrame(data, [\"features\"])\n",
    "\n",
    "bucketizer = Bucketizer(splits=splits, inputCol=\"features\", outputCol=\"bucketedFeatures\")\n",
    "\n",
    "# Transform original data into its bucket index.\n",
    "bucketedData = bucketizer.transform(dataFrame)\n",
    "\n",
    "print(\"Bucketizer output with %d buckets\" % (len(bucketizer.getSplits())-1))\n",
    "bucketedData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "dying-lender",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------------+\n",
      "|       vector|transformedVector|\n",
      "+-------------+-----------------+\n",
      "|[1.0,2.0,3.0]|    [0.0,2.0,6.0]|\n",
      "|[4.0,5.0,6.0]|   [0.0,5.0,12.0]|\n",
      "+-------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import ElementwiseProduct\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "# Create some vector data; also works for sparse vectors\n",
    "data = [(Vectors.dense([1.0, 2.0, 3.0]),), (Vectors.dense([4.0, 5.0, 6.0]),)]\n",
    "df = spark.createDataFrame(data, [\"vector\"])\n",
    "transformer = ElementwiseProduct(scalingVec=Vectors.dense([0.0, 1.0, 2.0]),\n",
    "                                 inputCol=\"vector\", outputCol=\"transformedVector\")\n",
    "# Batch transform the vectors to create new column:\n",
    "transformer.transform(df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "exempt-standard",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+-----+\n",
      "|  a|  b|out_a|out_b|\n",
      "+---+---+-----+-----+\n",
      "|1.0|NaN|  1.0|  4.0|\n",
      "|2.0|NaN|  2.0|  4.0|\n",
      "|NaN|3.0|  3.0|  3.0|\n",
      "|4.0|4.0|  4.0|  4.0|\n",
      "|5.0|5.0|  5.0|  5.0|\n",
      "+---+---+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (1.0, float(\"nan\")),\n",
    "    (2.0, float(\"nan\")),\n",
    "    (float(\"nan\"), 3.0),\n",
    "    (4.0, 4.0),\n",
    "    (5.0, 5.0)\n",
    "], [\"a\", \"b\"])\n",
    "\n",
    "imputer = Imputer(inputCols=[\"a\", \"b\"], outputCols=[\"out_a\", \"out_b\"])\n",
    "model = imputer.fit(df)\n",
    "\n",
    "model.transform(df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "laden-hopkins",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_data_frame = spark.createDataFrame([\n",
    "    (0, \"Hi I think pyspark is cool \",\"happy\"),\n",
    "    (1, \"All I want is a pyspark cluster\",\"indifferent\"),\n",
    "    (2, \"I finally understand how ML works\",\"fulfill\"),\n",
    "    (3, \"Yet another sentence about pyspark and ML\",\"indifferent\"),\n",
    "    (4, \"Why didn't I know about mllib before\",\"sad\"),\n",
    "    (5, \"Yes, I can\",\"happy\")\n",
    "], [\"id\", \"sentence\", \"sentiment\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "fifteen-serve",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, sentence: string, sentiment: string]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "macro-share",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------------------------+-----------+-------------------------------------------------+\n",
      "|id |sentence                                 |sentiment  |words                                            |\n",
      "+---+-----------------------------------------+-----------+-------------------------------------------------+\n",
      "|0  |Hi I think pyspark is cool               |happy      |[hi, i, think, pyspark, is, cool]                |\n",
      "|1  |All I want is a pyspark cluster          |indifferent|[all, i, want, is, a, pyspark, cluster]          |\n",
      "|2  |I finally understand how ML works        |fulfill    |[i, finally, understand, how, ml, works]         |\n",
      "|3  |Yet another sentence about pyspark and ML|indifferent|[yet, another, sentence, about, pyspark, and, ml]|\n",
      "|4  |Why didn't I know about mllib before     |sad        |[why, didn't, i, know, about, mllib, before]     |\n",
      "|5  |Yes, I can                               |happy      |[yes,, i, can]                                   |\n",
      "+---+-----------------------------------------+-----------+-------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "\n",
    "tokenized = tokenizer.transform(sentence_data_frame)\n",
    "tokenized.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "exact-trick",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------+-------------------------------------+\n",
      "|words                                            |meaningful_words                     |\n",
      "+-------------------------------------------------+-------------------------------------+\n",
      "|[hi, i, think, pyspark, is, cool]                |[hi, think, pyspark, cool]           |\n",
      "|[all, i, want, is, a, pyspark, cluster]          |[want, pyspark, cluster]             |\n",
      "|[i, finally, understand, how, ml, works]         |[finally, understand, ml, works]     |\n",
      "|[yet, another, sentence, about, pyspark, and, ml]|[yet, another, sentence, pyspark, ml]|\n",
      "|[why, didn't, i, know, about, mllib, before]     |[know, mllib]                        |\n",
      "|[yes,, i, can]                                   |[yes,]                               |\n",
      "+-------------------------------------------------+-------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"meaningful_words\")\n",
    "meaningful_data_frame = remover.transform(tokenized)\n",
    "\n",
    "meaningful_data_frame.select(\"words\",\"meaningful_words\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "apparent-cornwall",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----------+--------------------+--------------------+-------------+\n",
      "| id|            sentence|  sentiment|               words|    meaningful_words|categoryIndex|\n",
      "+---+--------------------+-----------+--------------------+--------------------+-------------+\n",
      "|  0|Hi I think pyspar...|      happy|[hi, i, think, py...|[hi, think, pyspa...|          0.0|\n",
      "|  1|All I want is a p...|indifferent|[all, i, want, is...|[want, pyspark, c...|          1.0|\n",
      "|  2|I finally underst...|    fulfill|[i, finally, unde...|[finally, underst...|          2.0|\n",
      "|  3|Yet another sente...|indifferent|[yet, another, se...|[yet, another, se...|          1.0|\n",
      "|  4|Why didn't I know...|        sad|[why, didn't, i, ...|       [know, mllib]|          3.0|\n",
      "|  5|          Yes, I can|      happy|      [yes,, i, can]|              [yes,]|          0.0|\n",
      "+---+--------------------+-----------+--------------------+--------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"sentiment\", outputCol=\"categoryIndex\")\n",
    "indexed = indexer.fit(meaningful_data_frame).transform(meaningful_data_frame)\n",
    "indexed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "billion-mileage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, sentence: string, sentiment: string, words: array<string>, meaningful_words: array<string>, categoryIndex: double]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "decimal-grenada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+-----------+-------+----+\n",
      "|sentence_id|happy|indifferent|fulfill| sad|\n",
      "+-----------+-----+-----------+-------+----+\n",
      "|          0| 0.01|       0.43|    0.3| 0.5|\n",
      "|          1|0.097|       0.21|    0.2| 0.9|\n",
      "|          2|  0.4|      0.329|   0.97| 0.4|\n",
      "|          3|  0.7|        0.4|    0.3|0.87|\n",
      "|          4| 0.34|        0.4|    0.3|0.78|\n",
      "|          5|  0.1|        0.3|   0.31|0.29|\n",
      "+-----------+-----+-----------+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment_data_frame = spark.createDataFrame([\n",
    "    (0, 0.01,0.43,0.3,0.5),\n",
    "    (1, 0.097,0.21,0.2,0.9),\n",
    "    (2, 0.4,0.329,0.97,0.4),\n",
    "    (3, 0.7,0.4,0.3,0.87),\n",
    "    (4, 0.34,0.4,0.3,0.78),\n",
    "    (5, 0.1,0.3,0.31,0.29)\n",
    "], [\"sentence_id\", \"happy\", \"indifferent\",\"fulfill\",\"sad\"])\n",
    "\n",
    "sentiment_data_frame.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "functioning-legislature",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|happy|\n",
      "+-----+\n",
      "| 0.01|\n",
      "|0.097|\n",
      "|  0.4|\n",
      "|  0.7|\n",
      "| 0.34|\n",
      "|  0.1|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "casted_data_frame = sentiment_data_frame.selectExpr(\"cast(happy as double)\")\n",
    "casted_data_frame.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "hundred-draft",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sentence_id: long (nullable = true)\n",
      " |-- happy: double (nullable = true)\n",
      " |-- indifferent: double (nullable = true)\n",
      " |-- fulfill: double (nullable = true)\n",
      " |-- sad: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment_data_frame.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "secure-cursor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "|label|            sentence|               words|         rawFeatures|            features|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "|  0.0|Hi I heard about ...|[hi, i, heard, ab...|(20,[6,8,13,16],[...|(20,[6,8,13,16],[...|\n",
      "|  0.0|I wish Java could...|[i, wish, java, c...|(20,[0,2,7,13,15,...|(20,[0,2,7,13,15,...|\n",
      "|  1.0|Logistic regressi...|[logistic, regres...|(20,[3,4,6,11,19]...|(20,[3,4,6,11,19]...|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "sentenceData = spark.createDataFrame([\n",
    "    (0.0, \"Hi I heard about Spark\"),\n",
    "    (0.0, \"I wish Java could use case classes\"),\n",
    "    (1.0, \"Logistic regression models are neat\")\n",
    "], [\"label\", \"sentence\"])\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(sentenceData)\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\n",
    "featurizedData = hashingTF.transform(wordsData)\n",
    "# alternatively, CountVectorizer can also be used to get term frequency vectors\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)\n",
    "\n",
    "rescaledData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solar-copyright",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
