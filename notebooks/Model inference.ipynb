{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c686250b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.keras\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from pyspark.sql.functions import col, struct\n",
    "\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53c3595",
   "metadata": {},
   "source": [
    " ### Move model from TesnorFlow to MLFlow registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32939824",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path =  \"/home/jovyan/dist-tf-model/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44068a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "restored_keras_model = tf.keras.models.load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7410aa2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run() as run:\n",
    "    mlflow.keras.log_model(restored_keras_model, \"models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83ea85e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id =\"425438f8a7b0471d9413684deeb63deb\"\n",
    "experiment_id = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d577652f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import pyspark.sql.functions \n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Model inference\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73783d8",
   "metadata": {},
   "source": [
    "### Define mlfloyw.pyfunc wrapper for the Model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9c44fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TIP: Create custom Python pyfunc model that transforms and predicts on inference data\n",
    "# Allows the inference pipeline to be independent of the model framework used in training pipeline\n",
    "class KerasCNNModelWrapper(mlflow.pyfunc.PythonModel):\n",
    "  def __init__(self, model_path):\n",
    "    self.model_path = model_path\n",
    "\n",
    "  def load_context(self, context):\n",
    "    # Load the Keras-native representation of the MLflow\n",
    "    # model\n",
    "    print(self.model_path)\n",
    "    self.model = mlflow.keras.load_model(\n",
    "        model_uri=self.model_path)\n",
    "\n",
    "  def predict(self, context, model_input):\n",
    "    import tensorflow as tf\n",
    "    import json\n",
    "\n",
    "    class_def = {\n",
    "      0: '212.teapot', \n",
    "      1: '234.tweezer', \n",
    "      2: '196.spaghetti', \n",
    "      3: '249.yo-yo', \n",
    "    }\n",
    "\n",
    "    model_input['origin'] = model_input['origin'].str.replace(\"dbfs:\",\"/dbfs\")\n",
    "    images = model_input['origin']\n",
    "\n",
    "    rtn_df = model_input.iloc[:,0:1]\n",
    "    rtn_df['prediction'] = None\n",
    "    rtn_df['probabilities'] = None\n",
    "\n",
    "    for index, row in model_input.iterrows():\n",
    "      image = np.round(np.array(Image.open(row['origin']).resize((224,224)),dtype=np.float32))\n",
    "      img = tf.reshape(image, shape=[-1, 224, 224, 3])\n",
    "      class_probs = self.model.predict(img)\n",
    "      classes = np.argmax(class_probs, axis=1)\n",
    "      class_prob_dict = dict()\n",
    "      for key, val in class_def.items():\n",
    "        class_prob_dict[val] = np.round(np.float(class_probs[0][int(key)]), 3).tolist()\n",
    "      rtn_df.loc[index,'prediction'] = classes[0]\n",
    "      rtn_df.loc[index,'probabilities'] = json.dumps(class_prob_dict)\n",
    "\n",
    "    return rtn_df[['prediction', 'probabilities']].values.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97eab4ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside MLflow Run with run_id `425438f8a7b0471d9413684deeb63deb` and experiment_id `0`\n"
     ]
    }
   ],
   "source": [
    "model_path = f\"file:/home/jovyan/mlruns/{experiment_id}/{run_id}/artifacts/models\"\n",
    "wrappedModel = KerasCNNModelWrapper(model_path)\n",
    "mlflow.pyfunc.log_model(\"pyfunc_model_v2\", python_model=wrappedModel)\n",
    "print(f\"Inside MLflow Run with run_id `{run_id}` and experiment_id `{experiment_id}`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4075e0dc",
   "metadata": {},
   "source": [
    "#### Test the model with mlflow.pyfunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29a6ba4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data. Using the same dataframe in this example\n",
    "images_df = spark.read.parquet( \"images_data/silver/augmented\")\n",
    "model_path = f\"file:/home/jovyan/mlruns/{experiment_id}/{run_id}/artifacts/models\"\n",
    "\n",
    "# Always use the Production version of the model from the registry\n",
    "mlflow_model_path = model_path\n",
    "\n",
    "# Load model as a Spark UDF.\n",
    "loaded_model = mlflow.pyfunc.spark_udf(spark, mlflow_model_path, result_type=ArrayType(StringType()))\n",
    "\n",
    "# Predict on a Spark DataFrame.\n",
    "scored_df = (images_df\n",
    "             .withColumn('origin', col(\"content\"))\n",
    "             .withColumn('my_predictions', loaded_model(struct(\"origin\")))\n",
    "             .drop(\"origin\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22124219",
   "metadata": {},
   "outputs": [
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 273, in dump_stream\n    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 81, in dump_stream\n    for batch in iterator:\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 266, in init_stream_yield_batches\n    for series in iterator:\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in mapper\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in <genexpr>\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 105, in <lambda>\n    verify_result_type(f(*a)), len(a[0])), arrow_return_type)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"/opt/conda/lib/python3.9/site-packages/mlflow/pyfunc/__init__.py\", line 856, in predict\n    result = model.predict(pdf)\n  File \"/opt/conda/lib/python3.9/site-packages/mlflow/pyfunc/__init__.py\", line 608, in predict\n    return self._model_impl.predict(data)\n  File \"/opt/conda/lib/python3.9/site-packages/mlflow/keras.py\", line 498, in predict\n    predicted = _predict(data)\n  File \"/opt/conda/lib/python3.9/site-packages/mlflow/keras.py\", line 485, in _predict\n    predicted = pd.DataFrame(self.keras_model.predict(data.values))\n  File \"/opt/conda/lib/python3.9/site-packages/keras/engine/training.py\", line 1751, in predict\n    tmp_batch_outputs = self.predict_function(iterator)\n  File \"/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\", line 885, in __call__\n    result = self._call(*args, **kwds)\n  File \"/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\", line 933, in _call\n    self._initialize(args, kwds, add_initializers_to=initializers)\n  File \"/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\", line 759, in _initialize\n    self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n  File \"/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\", line 3066, in _get_concrete_function_internal_garbage_collected\n    graph_function, _ = self._maybe_define_function(args, kwargs)\n  File \"/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\", line 3463, in _maybe_define_function\n    graph_function = self._create_graph_function(args, kwargs)\n  File \"/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\", line 3298, in _create_graph_function\n    func_graph_module.func_graph_from_py_func(\n  File \"/opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py\", line 1007, in func_graph_from_py_func\n    func_outputs = python_func(*func_args, **func_kwargs)\n  File \"/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\", line 668, in wrapped_fn\n    out = weak_wrapped_fn().__wrapped__(*args, **kwds)\n  File \"/opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py\", line 994, in wrapper\n    raise e.ag_error_metadata.to_exception(e)\nValueError: in user code:\n\n    /opt/conda/lib/python3.9/site-packages/keras/engine/training.py:1586 predict_function  *\n        return step_function(self, iterator)\n    /opt/conda/lib/python3.9/site-packages/keras/engine/training.py:1576 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:1286 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:2849 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:3632 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /opt/conda/lib/python3.9/site-packages/keras/engine/training.py:1569 run_step  **\n        outputs = model.predict_step(data)\n    /opt/conda/lib/python3.9/site-packages/keras/engine/training.py:1537 predict_step\n        return self(x, training=False)\n    /opt/conda/lib/python3.9/site-packages/keras/engine/base_layer.py:1037 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    /opt/conda/lib/python3.9/site-packages/keras/engine/sequential.py:369 call\n        return super(Sequential, self).call(inputs, training=training, mask=mask)\n    /opt/conda/lib/python3.9/site-packages/keras/engine/functional.py:414 call\n        return self._run_internal_graph(\n    /opt/conda/lib/python3.9/site-packages/keras/engine/functional.py:550 _run_internal_graph\n        outputs = node.layer(*args, **kwargs)\n    /opt/conda/lib/python3.9/site-packages/keras/engine/base_layer.py:1037 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    /opt/conda/lib/python3.9/site-packages/keras/engine/functional.py:414 call\n        return self._run_internal_graph(\n    /opt/conda/lib/python3.9/site-packages/keras/engine/functional.py:550 _run_internal_graph\n        outputs = node.layer(*args, **kwargs)\n    /opt/conda/lib/python3.9/site-packages/keras/engine/base_layer.py:1020 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    /opt/conda/lib/python3.9/site-packages/keras/engine/input_spec.py:229 assert_input_compatibility\n        raise ValueError('Input ' + str(input_index) + ' of layer ' +\n\n    ValueError: Input 0 of layer Conv1 is incompatible with the layer: : expected min_ndim=4, found ndim=2. Full shape received: (None, 1)\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-0a957b79e2c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscored_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    484\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 273, in dump_stream\n    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 81, in dump_stream\n    for batch in iterator:\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 266, in init_stream_yield_batches\n    for series in iterator:\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in mapper\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in <genexpr>\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 105, in <lambda>\n    verify_result_type(f(*a)), len(a[0])), arrow_return_type)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"/opt/conda/lib/python3.9/site-packages/mlflow/pyfunc/__init__.py\", line 856, in predict\n    result = model.predict(pdf)\n  File \"/opt/conda/lib/python3.9/site-packages/mlflow/pyfunc/__init__.py\", line 608, in predict\n    return self._model_impl.predict(data)\n  File \"/opt/conda/lib/python3.9/site-packages/mlflow/keras.py\", line 498, in predict\n    predicted = _predict(data)\n  File \"/opt/conda/lib/python3.9/site-packages/mlflow/keras.py\", line 485, in _predict\n    predicted = pd.DataFrame(self.keras_model.predict(data.values))\n  File \"/opt/conda/lib/python3.9/site-packages/keras/engine/training.py\", line 1751, in predict\n    tmp_batch_outputs = self.predict_function(iterator)\n  File \"/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\", line 885, in __call__\n    result = self._call(*args, **kwds)\n  File \"/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\", line 933, in _call\n    self._initialize(args, kwds, add_initializers_to=initializers)\n  File \"/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\", line 759, in _initialize\n    self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n  File \"/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\", line 3066, in _get_concrete_function_internal_garbage_collected\n    graph_function, _ = self._maybe_define_function(args, kwargs)\n  File \"/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\", line 3463, in _maybe_define_function\n    graph_function = self._create_graph_function(args, kwargs)\n  File \"/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\", line 3298, in _create_graph_function\n    func_graph_module.func_graph_from_py_func(\n  File \"/opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py\", line 1007, in func_graph_from_py_func\n    func_outputs = python_func(*func_args, **func_kwargs)\n  File \"/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\", line 668, in wrapped_fn\n    out = weak_wrapped_fn().__wrapped__(*args, **kwds)\n  File \"/opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py\", line 994, in wrapper\n    raise e.ag_error_metadata.to_exception(e)\nValueError: in user code:\n\n    /opt/conda/lib/python3.9/site-packages/keras/engine/training.py:1586 predict_function  *\n        return step_function(self, iterator)\n    /opt/conda/lib/python3.9/site-packages/keras/engine/training.py:1576 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:1286 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:2849 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:3632 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /opt/conda/lib/python3.9/site-packages/keras/engine/training.py:1569 run_step  **\n        outputs = model.predict_step(data)\n    /opt/conda/lib/python3.9/site-packages/keras/engine/training.py:1537 predict_step\n        return self(x, training=False)\n    /opt/conda/lib/python3.9/site-packages/keras/engine/base_layer.py:1037 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    /opt/conda/lib/python3.9/site-packages/keras/engine/sequential.py:369 call\n        return super(Sequential, self).call(inputs, training=training, mask=mask)\n    /opt/conda/lib/python3.9/site-packages/keras/engine/functional.py:414 call\n        return self._run_internal_graph(\n    /opt/conda/lib/python3.9/site-packages/keras/engine/functional.py:550 _run_internal_graph\n        outputs = node.layer(*args, **kwargs)\n    /opt/conda/lib/python3.9/site-packages/keras/engine/base_layer.py:1037 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    /opt/conda/lib/python3.9/site-packages/keras/engine/functional.py:414 call\n        return self._run_internal_graph(\n    /opt/conda/lib/python3.9/site-packages/keras/engine/functional.py:550 _run_internal_graph\n        outputs = node.layer(*args, **kwargs)\n    /opt/conda/lib/python3.9/site-packages/keras/engine/base_layer.py:1020 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    /opt/conda/lib/python3.9/site-packages/keras/engine/input_spec.py:229 assert_input_compatibility\n        raise ValueError('Input ' + str(input_index) + ' of layer ' +\n\n    ValueError: Input 0 of layer Conv1 is incompatible with the layer: : expected min_ndim=4, found ndim=2. Full shape received: (None, 1)\n\n"
     ]
    }
   ],
   "source": [
    "scored_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00707c73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
